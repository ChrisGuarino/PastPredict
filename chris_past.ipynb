{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "df = pdr.get_data_yahoo(\"IBM\", start=\"2019-01-01\", end=\"2024-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>163.750000</td>\n",
       "      <td>164.179993</td>\n",
       "      <td>162.830002</td>\n",
       "      <td>163.550003</td>\n",
       "      <td>162.072403</td>\n",
       "      <td>2525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>163.960007</td>\n",
       "      <td>163.960007</td>\n",
       "      <td>163.399994</td>\n",
       "      <td>163.750000</td>\n",
       "      <td>162.270599</td>\n",
       "      <td>2071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>163.139999</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>162.679993</td>\n",
       "      <td>163.460007</td>\n",
       "      <td>161.983231</td>\n",
       "      <td>3234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>162.229996</td>\n",
       "      <td>163.309998</td>\n",
       "      <td>162.050003</td>\n",
       "      <td>163.210007</td>\n",
       "      <td>161.735489</td>\n",
       "      <td>1772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>161.100006</td>\n",
       "      <td>162.410004</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>162.139999</td>\n",
       "      <td>160.675140</td>\n",
       "      <td>2439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>114.397705</td>\n",
       "      <td>115.267685</td>\n",
       "      <td>113.747612</td>\n",
       "      <td>114.560226</td>\n",
       "      <td>88.952118</td>\n",
       "      <td>4982726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>112.332695</td>\n",
       "      <td>113.604210</td>\n",
       "      <td>111.539200</td>\n",
       "      <td>112.954109</td>\n",
       "      <td>87.705025</td>\n",
       "      <td>3923755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>109.856598</td>\n",
       "      <td>112.323135</td>\n",
       "      <td>109.407265</td>\n",
       "      <td>112.160614</td>\n",
       "      <td>87.088921</td>\n",
       "      <td>4683779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>109.493309</td>\n",
       "      <td>109.827919</td>\n",
       "      <td>107.734222</td>\n",
       "      <td>107.944550</td>\n",
       "      <td>83.815269</td>\n",
       "      <td>4546648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>107.084129</td>\n",
       "      <td>110.879539</td>\n",
       "      <td>106.778206</td>\n",
       "      <td>110.143402</td>\n",
       "      <td>85.522606</td>\n",
       "      <td>4434935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2023-12-29  163.750000  164.179993  162.830002  163.550003  162.072403   \n",
       "2023-12-28  163.960007  163.960007  163.399994  163.750000  162.270599   \n",
       "2023-12-27  163.139999  163.639999  162.679993  163.460007  161.983231   \n",
       "2023-12-26  162.229996  163.309998  162.050003  163.210007  161.735489   \n",
       "2023-12-22  161.100006  162.410004  161.000000  162.139999  160.675140   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2019-01-08  114.397705  115.267685  113.747612  114.560226   88.952118   \n",
       "2019-01-07  112.332695  113.604210  111.539200  112.954109   87.705025   \n",
       "2019-01-04  109.856598  112.323135  109.407265  112.160614   87.088921   \n",
       "2019-01-03  109.493309  109.827919  107.734222  107.944550   83.815269   \n",
       "2019-01-02  107.084129  110.879539  106.778206  110.143402   85.522606   \n",
       "\n",
       "             Volume  \n",
       "Date                 \n",
       "2023-12-29  2525600  \n",
       "2023-12-28  2071300  \n",
       "2023-12-27  3234600  \n",
       "2023-12-26  1772400  \n",
       "2023-12-22  2439800  \n",
       "...             ...  \n",
       "2019-01-08  4982726  \n",
       "2019-01-07  3923755  \n",
       "2019-01-04  4683779  \n",
       "2019-01-03  4546648  \n",
       "2019-01-02  4434935  \n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reversed_df = df.iloc[::-1]\n",
    "reversed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(reversed_df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Sequences for RNN \n",
    "The way I think it makes sense right now is to have the features of the network be all available feature columns (Open, High, Low, Close, Adj Close, and Volume) and then as the label we are targeting be the next day's Abj Close. \n",
    "\n",
    "Once this is working the way that it needs to we can then set up a label for predicitng the previous day's. I think that it should be the same work flow. For a feature datapoint we are just trying to predict Adj Close -1 vs Adj Close +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length][4] \n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 5\n",
    "X, y = create_sequences(scaled_features, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1252, 5, 6) (1252,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.8\n",
    "split = int(split_fraction * len(X))\n",
    "\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training set: (1001, 5, 6)\n",
      "y Training set: (1001,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X Training set: {X_train.shape}\\ny Training set: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Torch Tensors and set up Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64  \n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.1027,  3.0329,  3.1098,  3.0698,  3.0825, -0.9514],\n",
       "         [ 3.0272,  3.0031,  3.0445,  3.0432,  3.0638, -0.5675],\n",
       "         [ 2.9435,  2.9724,  2.9873,  3.0203,  3.0476, -1.0500],\n",
       "         [ 2.8395,  2.8886,  2.8920,  2.9221,  2.9785, -0.8298],\n",
       "         [ 2.7925,  2.7647,  2.7586,  2.7973,  2.8907, -0.6506]]),\n",
       " tensor(2.8436))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the RNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define instance of model/training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (rnn): RNN(6, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    print(\"Running on GPU\")\n",
    "else: print(\"Running on CPU\")\n",
    "\n",
    "#Model Define\n",
    "model = SimpleRNN(input_size=6, hidden_size=20, output_size=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.3137\n",
      "Epoch [2/100], Loss: 0.2532\n",
      "Epoch [3/100], Loss: 0.2782\n",
      "Epoch [4/100], Loss: 0.1838\n",
      "Epoch [5/100], Loss: 0.0619\n",
      "Epoch [6/100], Loss: 0.0451\n",
      "Epoch [7/100], Loss: 0.0295\n",
      "Epoch [8/100], Loss: 0.0513\n",
      "Epoch [9/100], Loss: 0.0489\n",
      "Epoch [10/100], Loss: 0.0244\n",
      "Epoch [11/100], Loss: 0.0148\n",
      "Epoch [12/100], Loss: 0.0267\n",
      "Epoch [13/100], Loss: 0.0154\n",
      "Epoch [14/100], Loss: 0.0284\n",
      "Epoch [15/100], Loss: 0.0175\n",
      "Epoch [16/100], Loss: 0.0078\n",
      "Epoch [17/100], Loss: 0.0179\n",
      "Epoch [18/100], Loss: 0.0169\n",
      "Epoch [19/100], Loss: 0.0119\n",
      "Epoch [20/100], Loss: 0.0177\n",
      "Epoch [21/100], Loss: 0.0109\n",
      "Epoch [22/100], Loss: 0.0145\n",
      "Epoch [23/100], Loss: 0.0194\n",
      "Epoch [24/100], Loss: 0.0145\n",
      "Epoch [25/100], Loss: 0.0179\n",
      "Epoch [26/100], Loss: 0.0120\n",
      "Epoch [27/100], Loss: 0.0137\n",
      "Epoch [28/100], Loss: 0.0118\n",
      "Epoch [29/100], Loss: 0.0100\n",
      "Epoch [30/100], Loss: 0.0105\n",
      "Epoch [31/100], Loss: 0.0089\n",
      "Epoch [32/100], Loss: 0.0115\n",
      "Epoch [33/100], Loss: 0.0065\n",
      "Epoch [34/100], Loss: 0.0124\n",
      "Epoch [35/100], Loss: 0.0093\n",
      "Epoch [36/100], Loss: 0.0077\n",
      "Epoch [37/100], Loss: 0.0116\n",
      "Epoch [38/100], Loss: 0.0065\n",
      "Epoch [39/100], Loss: 0.0077\n",
      "Epoch [40/100], Loss: 0.0061\n",
      "Epoch [41/100], Loss: 0.0075\n",
      "Epoch [42/100], Loss: 0.0104\n",
      "Epoch [43/100], Loss: 0.0139\n",
      "Epoch [44/100], Loss: 0.0104\n",
      "Epoch [45/100], Loss: 0.0038\n",
      "Epoch [46/100], Loss: 0.0166\n",
      "Epoch [47/100], Loss: 0.0057\n",
      "Epoch [48/100], Loss: 0.0068\n",
      "Epoch [49/100], Loss: 0.0065\n",
      "Epoch [50/100], Loss: 0.0074\n",
      "Epoch [51/100], Loss: 0.0052\n",
      "Epoch [52/100], Loss: 0.0088\n",
      "Epoch [53/100], Loss: 0.0105\n",
      "Epoch [54/100], Loss: 0.0048\n",
      "Epoch [55/100], Loss: 0.0059\n",
      "Epoch [56/100], Loss: 0.0076\n",
      "Epoch [57/100], Loss: 0.0097\n",
      "Epoch [58/100], Loss: 0.0092\n",
      "Epoch [59/100], Loss: 0.0068\n",
      "Epoch [60/100], Loss: 0.0036\n",
      "Epoch [61/100], Loss: 0.0032\n",
      "Epoch [62/100], Loss: 0.0061\n",
      "Epoch [63/100], Loss: 0.0085\n",
      "Epoch [64/100], Loss: 0.0034\n",
      "Epoch [65/100], Loss: 0.0153\n",
      "Epoch [66/100], Loss: 0.0147\n",
      "Epoch [67/100], Loss: 0.0059\n",
      "Epoch [68/100], Loss: 0.0100\n",
      "Epoch [69/100], Loss: 0.0105\n",
      "Epoch [70/100], Loss: 0.0031\n",
      "Epoch [71/100], Loss: 0.0067\n",
      "Epoch [72/100], Loss: 0.0052\n",
      "Epoch [73/100], Loss: 0.0067\n",
      "Epoch [74/100], Loss: 0.0066\n",
      "Epoch [75/100], Loss: 0.0036\n",
      "Epoch [76/100], Loss: 0.0111\n",
      "Epoch [77/100], Loss: 0.0063\n",
      "Epoch [78/100], Loss: 0.0103\n",
      "Epoch [79/100], Loss: 0.0059\n",
      "Epoch [80/100], Loss: 0.0068\n",
      "Epoch [81/100], Loss: 0.0114\n",
      "Epoch [82/100], Loss: 0.0045\n",
      "Epoch [83/100], Loss: 0.0091\n",
      "Epoch [84/100], Loss: 0.0069\n",
      "Epoch [85/100], Loss: 0.0092\n",
      "Epoch [86/100], Loss: 0.0097\n",
      "Epoch [87/100], Loss: 0.0097\n",
      "Epoch [88/100], Loss: 0.0041\n",
      "Epoch [89/100], Loss: 0.0046\n",
      "Epoch [90/100], Loss: 0.0082\n",
      "Epoch [91/100], Loss: 0.0085\n",
      "Epoch [92/100], Loss: 0.0026\n",
      "Epoch [93/100], Loss: 0.0040\n",
      "Epoch [94/100], Loss: 0.0054\n",
      "Epoch [95/100], Loss: 0.0051\n",
      "Epoch [96/100], Loss: 0.0061\n",
      "Epoch [97/100], Loss: 0.0102\n",
      "Epoch [98/100], Loss: 0.0031\n",
      "Epoch [99/100], Loss: 0.0054\n",
      "Epoch [100/100], Loss: 0.0050\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels.unsqueeze(-1))\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.0753\n",
      "Mean Squared Error: 0.0094\n",
      "Root Mean Squared Error: 0.0968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "actuals = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        actuals.extend(labels.cpu().numpy())\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "actuals = np.array(actuals)\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics? \n",
    "Not sure what we want to use for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6965378 , -0.75888395, -0.6341917 , -0.6524394 , -0.6488905 ,\n",
       "       -0.61999846, -0.6179707 , -0.6701801 , -0.677784  , -0.68741417,\n",
       "       -0.6919758 , -0.68792176, -0.63165736, -0.71073055, -0.7031276 ,\n",
       "       -0.70262057, -0.7381024 , -0.80450404, -0.80044854, -0.7938587 ,\n",
       "       -0.7538153 , -0.67575574, -0.7102225 , -0.6433153 , -0.59871036,\n",
       "       -0.681331  , -0.70667505, -0.739116  , -0.6722068 , -0.6828526 ,\n",
       "       -0.67828906, -0.69856507, -0.6742351 , -0.6179707 , -0.62405396,\n",
       "       -0.51558167, -0.51152664, -0.53839093, -0.5829765 , -0.59399813,\n",
       "       -0.70120424, -0.7913781 , -0.71523094, -0.7868692 , -0.67916304,\n",
       "       -0.7057126 , -0.7743448 , -0.7588149 , -0.7798564 , -0.8489892 ,\n",
       "       -0.7733436 , -0.7648275 , -0.37157038, -0.3269833 , -0.37507656,\n",
       "       -0.33900613, -0.4206643 , -0.49380538, -0.5584303 , -0.41314888,\n",
       "       -0.3274849 , -0.37607774, -0.39260992, -0.29391894, -0.20575148,\n",
       "       -0.314961  , -0.299431  , -0.31896925, -0.3931125 , -0.37357375,\n",
       "       -0.3830931 , -0.3284866 , -0.3660578 , -0.3670605 , -0.3530338 ,\n",
       "       -0.29341885, -0.29592434, -0.29692402, -0.22428507, -0.34702316,\n",
       "       -0.4487187 , -0.42867985, -0.66162866, -0.77284205, -0.70120424,\n",
       "       -0.73376656, -0.8399714 , -0.91962624, -0.9787386 , -0.9997802 ,\n",
       "       -0.7618209 , -0.7653276 , -0.8279491 , -0.725752  , -0.7898763 ,\n",
       "       -0.88205403, -0.915617  , -0.6881788 , -0.7943837 , -0.671147  ,\n",
       "       -0.47226274, -0.6030149 , -0.5228033 , -0.5213179 , -0.19997646,\n",
       "       -0.04797299, -0.15096056, -0.07520466, -0.0202457 ,  0.00352054,\n",
       "       -0.04450609, -0.06233127, -0.04500321, -0.07669154, -0.07966281,\n",
       "       -0.08213846, -0.40694383, -0.38416687, -0.3945651 , -0.42328313,\n",
       "       -0.49755168, -0.5356772 , -0.5921225 , -0.5307264 , -0.49062085,\n",
       "       -0.48269776, -0.5480554 , -0.56489086, -0.6629266 , -0.6322266 ,\n",
       "       -0.6322266 , -0.6401501 , -0.5911323 , -0.5985592 , -0.6158883 ,\n",
       "       -0.7035268 , -0.7381864 , -0.8089895 , -0.7990859 , -0.768884  ,\n",
       "       -0.8129511 , -0.7594765 , -0.81938726, -0.89019036, -0.9441606 ,\n",
       "       -0.98030454, -0.9208884 , -1.1397355 , -1.2031131 , -1.075371  ,\n",
       "       -1.069428  , -1.0313034 , -0.94118977, -0.93574333, -0.7396718 ,\n",
       "       -0.73472   , -0.80057085, -0.8401827 , -0.7629429 , -0.8362217 ,\n",
       "       -0.89019036, -0.98377097, -0.7906702 , -0.78967893, -0.7381864 ,\n",
       "       -0.75580084, -0.62172794, -0.62808955, -0.66038436, -0.61292046,\n",
       "       -0.6271097 , -0.6868077 , -0.6677233 , -0.7073581 , -0.642769  ,\n",
       "       -0.6187939 , -0.6946358 , -0.62417424, -0.6838712 , -0.38881242,\n",
       "       -0.44948876, -0.42746887, -0.4553602 , -0.49254668, -0.5370756 ,\n",
       "       -0.4744436 , -0.47982442, -0.5042896 , -0.46269867, -0.4935275 ,\n",
       "       -0.4788456 , -0.58649665, -0.64423794, -0.6775091 , -0.62955755,\n",
       "       -0.6804461 , -0.66723466, -0.56986004, -0.6598942 , -0.61634564,\n",
       "       -0.6300477 , -0.668213  , -0.69952995, -0.71078426, -0.7244858 ,\n",
       "       -0.7523767 , -0.88057756, -0.8673652 , -0.7880951 , -0.7440579 ,\n",
       "       -0.7171454 , -0.6794668 , -0.73182476, -0.68093574, -0.65402323,\n",
       "       -0.666745  , -0.67702144, -0.746015  , -0.7381864 , -0.7039334 ,\n",
       "       -0.7367184 , -0.8125618 , -0.7616728 , -0.83360237, -0.9344026 ,\n",
       "       -0.9481037 , -0.9735497 , -0.8972151 , -0.9344583 , -0.95187086,\n",
       "       -1.0045923 , -0.989114  , -0.99104875, -0.9934673 , -0.9963694 ,\n",
       "       -1.0108799 , -1.0805311 , -1.063117  , -1.5646979 , -1.5018178 ,\n",
       "       -1.5806583 , -1.608227  , -1.6029079 , -1.6677216 , -1.6159667 ,\n",
       "       -1.6000056 , -1.6532108 , -1.6948081 , -1.7760667 , -1.816211  ,\n",
       "       -2.0295167 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58056784],\n",
       "       [-0.7286117 ],\n",
       "       [-0.60630035],\n",
       "       [-0.59714234],\n",
       "       [-0.61857355],\n",
       "       [-0.5639907 ],\n",
       "       [-0.5369048 ],\n",
       "       [-0.576174  ],\n",
       "       [-0.6669287 ],\n",
       "       [-0.6495438 ],\n",
       "       [-0.6728127 ],\n",
       "       [-0.6065775 ],\n",
       "       [-0.6200795 ],\n",
       "       [-0.6780084 ],\n",
       "       [-0.6862066 ],\n",
       "       [-0.6772333 ],\n",
       "       [-0.7089276 ],\n",
       "       [-0.7429358 ],\n",
       "       [-0.7444322 ],\n",
       "       [-0.73398966],\n",
       "       [-0.8021116 ],\n",
       "       [-0.63151777],\n",
       "       [-0.6528645 ],\n",
       "       [-0.56816906],\n",
       "       [-0.5412107 ],\n",
       "       [-0.6182744 ],\n",
       "       [-0.6640591 ],\n",
       "       [-0.69886684],\n",
       "       [-0.66517186],\n",
       "       [-0.61094874],\n",
       "       [-0.6578084 ],\n",
       "       [-0.6330788 ],\n",
       "       [-0.67265594],\n",
       "       [-0.5827613 ],\n",
       "       [-0.5403139 ],\n",
       "       [-0.49558994],\n",
       "       [-0.4500351 ],\n",
       "       [-0.42878187],\n",
       "       [-0.48165432],\n",
       "       [-0.50793916],\n",
       "       [-0.6269009 ],\n",
       "       [-0.7327473 ],\n",
       "       [-0.6940999 ],\n",
       "       [-0.72503006],\n",
       "       [-0.6559052 ],\n",
       "       [-0.61103725],\n",
       "       [-0.7084691 ],\n",
       "       [-0.7000237 ],\n",
       "       [-0.7467975 ],\n",
       "       [-0.80232084],\n",
       "       [-0.8208067 ],\n",
       "       [-0.7275796 ],\n",
       "       [-0.64431673],\n",
       "       [-0.19063011],\n",
       "       [-0.23194805],\n",
       "       [-0.31980324],\n",
       "       [-0.24005881],\n",
       "       [-0.4613688 ],\n",
       "       [-0.4703203 ],\n",
       "       [-0.4392271 ],\n",
       "       [-0.29934895],\n",
       "       [-0.30253777],\n",
       "       [-0.3274834 ],\n",
       "       [-0.28130737],\n",
       "       [-0.08232846],\n",
       "       [-0.21578555],\n",
       "       [-0.24522781],\n",
       "       [-0.28647134],\n",
       "       [-0.3328273 ],\n",
       "       [-0.30530018],\n",
       "       [-0.35841328],\n",
       "       [-0.25279206],\n",
       "       [-0.2870425 ],\n",
       "       [-0.36285084],\n",
       "       [-0.3109035 ],\n",
       "       [-0.26380223],\n",
       "       [-0.18656078],\n",
       "       [-0.20684725],\n",
       "       [-0.15458667],\n",
       "       [-0.263514  ],\n",
       "       [-0.44063357],\n",
       "       [-0.40652966],\n",
       "       [-0.5290648 ],\n",
       "       [-0.71415627],\n",
       "       [-0.72869635],\n",
       "       [-0.6532099 ],\n",
       "       [-0.7041406 ],\n",
       "       [-0.92280203],\n",
       "       [-0.8945379 ],\n",
       "       [-0.8933618 ],\n",
       "       [-0.73756105],\n",
       "       [-0.6760341 ],\n",
       "       [-0.68259823],\n",
       "       [-0.69099814],\n",
       "       [-0.67649174],\n",
       "       [-0.8149139 ],\n",
       "       [-0.914346  ],\n",
       "       [-0.7590856 ],\n",
       "       [-0.71414125],\n",
       "       [-0.6598505 ],\n",
       "       [-0.457683  ],\n",
       "       [-0.4810792 ],\n",
       "       [-0.54152995],\n",
       "       [-0.3531213 ],\n",
       "       [-0.2468327 ],\n",
       "       [ 0.01178806],\n",
       "       [-0.07199197],\n",
       "       [-0.11699168],\n",
       "       [-0.0301852 ],\n",
       "       [ 0.04664113],\n",
       "       [ 0.0729735 ],\n",
       "       [ 0.01055266],\n",
       "       [ 0.02364297],\n",
       "       [ 0.01575871],\n",
       "       [ 0.02430791],\n",
       "       [-0.06251431],\n",
       "       [-0.44313106],\n",
       "       [-0.46107507],\n",
       "       [-0.42255256],\n",
       "       [-0.40572664],\n",
       "       [-0.39221433],\n",
       "       [-0.46104902],\n",
       "       [-0.48057482],\n",
       "       [-0.4888287 ],\n",
       "       [-0.44535246],\n",
       "       [-0.44218013],\n",
       "       [-0.42577678],\n",
       "       [-0.4919289 ],\n",
       "       [-0.49113908],\n",
       "       [-0.6213693 ],\n",
       "       [-0.5910438 ],\n",
       "       [-0.5518337 ],\n",
       "       [-0.5416289 ],\n",
       "       [-0.51042277],\n",
       "       [-0.58587056],\n",
       "       [-0.56097   ],\n",
       "       [-0.6754945 ],\n",
       "       [-0.76604   ],\n",
       "       [-0.7467985 ],\n",
       "       [-0.72056806],\n",
       "       [-0.71919876],\n",
       "       [-0.71151835],\n",
       "       [-0.70297253],\n",
       "       [-0.78094995],\n",
       "       [-0.86667854],\n",
       "       [-0.90907586],\n",
       "       [-0.81757927],\n",
       "       [-0.9959543 ],\n",
       "       [-1.1845349 ],\n",
       "       [-1.1083496 ],\n",
       "       [-0.9885578 ],\n",
       "       [-0.9820285 ],\n",
       "       [-0.8627146 ],\n",
       "       [-0.78799725],\n",
       "       [-0.7346161 ],\n",
       "       [-0.64690423],\n",
       "       [-0.65701145],\n",
       "       [-0.8124693 ],\n",
       "       [-0.74280775],\n",
       "       [-0.73450285],\n",
       "       [-0.8527111 ],\n",
       "       [-0.884191  ],\n",
       "       [-0.8602317 ],\n",
       "       [-0.72797704],\n",
       "       [-0.71991384],\n",
       "       [-0.56573653],\n",
       "       [-0.55219185],\n",
       "       [-0.5792495 ],\n",
       "       [-0.54292315],\n",
       "       [-0.52612597],\n",
       "       [-0.5141398 ],\n",
       "       [-0.6297232 ],\n",
       "       [-0.6301372 ],\n",
       "       [-0.59353703],\n",
       "       [-0.57589257],\n",
       "       [-0.4954798 ],\n",
       "       [-0.5982366 ],\n",
       "       [-0.61473674],\n",
       "       [-0.63617504],\n",
       "       [-0.71325195],\n",
       "       [-0.3178151 ],\n",
       "       [-0.34941038],\n",
       "       [-0.3678818 ],\n",
       "       [-0.37028602],\n",
       "       [-0.44874683],\n",
       "       [-0.4417002 ],\n",
       "       [-0.40098178],\n",
       "       [-0.38210183],\n",
       "       [-0.35929313],\n",
       "       [-0.36562508],\n",
       "       [-0.392695  ],\n",
       "       [-0.4849209 ],\n",
       "       [-0.56600714],\n",
       "       [-0.582918  ],\n",
       "       [-0.55053747],\n",
       "       [-0.5433629 ],\n",
       "       [-0.6129004 ],\n",
       "       [-0.5063389 ],\n",
       "       [-0.5740696 ],\n",
       "       [-0.5589122 ],\n",
       "       [-0.51066506],\n",
       "       [-0.58451855],\n",
       "       [-0.6215443 ],\n",
       "       [-0.6734824 ],\n",
       "       [-0.652045  ],\n",
       "       [-0.65440094],\n",
       "       [-0.78030115],\n",
       "       [-0.89448166],\n",
       "       [-0.74294615],\n",
       "       [-0.6292336 ],\n",
       "       [-0.63152754],\n",
       "       [-0.55635965],\n",
       "       [-0.56807375],\n",
       "       [-0.64919776],\n",
       "       [-0.5935946 ],\n",
       "       [-0.5521337 ],\n",
       "       [-0.55902034],\n",
       "       [-0.6288047 ],\n",
       "       [-0.6875073 ],\n",
       "       [-0.6286143 ],\n",
       "       [-0.66657543],\n",
       "       [-0.6892868 ],\n",
       "       [-0.7153335 ],\n",
       "       [-0.71279174],\n",
       "       [-0.82435983],\n",
       "       [-0.86415225],\n",
       "       [-0.96800554],\n",
       "       [-0.8881666 ],\n",
       "       [-0.784789  ],\n",
       "       [-0.8472624 ],\n",
       "       [-0.92199755],\n",
       "       [-0.86860615],\n",
       "       [-0.91437054],\n",
       "       [-0.92944694],\n",
       "       [-0.91355187],\n",
       "       [-0.9911176 ],\n",
       "       [-1.0034045 ],\n",
       "       [-1.038363  ],\n",
       "       [-0.8491382 ],\n",
       "       [-1.4616481 ],\n",
       "       [-1.4470228 ],\n",
       "       [-1.6720991 ],\n",
       "       [-1.5554272 ],\n",
       "       [-1.5461092 ],\n",
       "       [-1.5952884 ],\n",
       "       [-1.5184087 ],\n",
       "       [-1.5613682 ],\n",
       "       [-1.5525916 ],\n",
       "       [-1.6144068 ],\n",
       "       [-1.7318054 ],\n",
       "       [-1.8371093 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictpast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
