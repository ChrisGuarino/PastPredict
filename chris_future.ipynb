{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "from pandas_datareader import data as pdr\n",
    "\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "df = pdr.get_data_yahoo(\"IBM\", start=\"2019-01-01\", end=\"2024-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are these features enough? \n",
    "Maybe we want to consier deriving other indicators (moving averages, RSI, MACD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>107.084129</td>\n",
       "      <td>110.879539</td>\n",
       "      <td>106.778206</td>\n",
       "      <td>110.143402</td>\n",
       "      <td>85.522606</td>\n",
       "      <td>4434935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>109.493309</td>\n",
       "      <td>109.827919</td>\n",
       "      <td>107.734222</td>\n",
       "      <td>107.944550</td>\n",
       "      <td>83.815262</td>\n",
       "      <td>4546648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>109.856598</td>\n",
       "      <td>112.323135</td>\n",
       "      <td>109.407265</td>\n",
       "      <td>112.160614</td>\n",
       "      <td>87.088905</td>\n",
       "      <td>4683779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>112.332695</td>\n",
       "      <td>113.604210</td>\n",
       "      <td>111.539200</td>\n",
       "      <td>112.954109</td>\n",
       "      <td>87.705017</td>\n",
       "      <td>3923755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>114.397705</td>\n",
       "      <td>115.267685</td>\n",
       "      <td>113.747612</td>\n",
       "      <td>114.560226</td>\n",
       "      <td>88.952118</td>\n",
       "      <td>4982726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-22</th>\n",
       "      <td>161.100006</td>\n",
       "      <td>162.410004</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>162.139999</td>\n",
       "      <td>160.675140</td>\n",
       "      <td>2439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-26</th>\n",
       "      <td>162.229996</td>\n",
       "      <td>163.309998</td>\n",
       "      <td>162.050003</td>\n",
       "      <td>163.210007</td>\n",
       "      <td>161.735489</td>\n",
       "      <td>1772400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-27</th>\n",
       "      <td>163.139999</td>\n",
       "      <td>163.639999</td>\n",
       "      <td>162.679993</td>\n",
       "      <td>163.460007</td>\n",
       "      <td>161.983231</td>\n",
       "      <td>3234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-28</th>\n",
       "      <td>163.960007</td>\n",
       "      <td>163.960007</td>\n",
       "      <td>163.399994</td>\n",
       "      <td>163.750000</td>\n",
       "      <td>162.270599</td>\n",
       "      <td>2071300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-12-29</th>\n",
       "      <td>163.750000</td>\n",
       "      <td>164.179993</td>\n",
       "      <td>162.830002</td>\n",
       "      <td>163.550003</td>\n",
       "      <td>162.072403</td>\n",
       "      <td>2525600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2019-01-02  107.084129  110.879539  106.778206  110.143402   85.522606   \n",
       "2019-01-03  109.493309  109.827919  107.734222  107.944550   83.815262   \n",
       "2019-01-04  109.856598  112.323135  109.407265  112.160614   87.088905   \n",
       "2019-01-07  112.332695  113.604210  111.539200  112.954109   87.705017   \n",
       "2019-01-08  114.397705  115.267685  113.747612  114.560226   88.952118   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2023-12-22  161.100006  162.410004  161.000000  162.139999  160.675140   \n",
       "2023-12-26  162.229996  163.309998  162.050003  163.210007  161.735489   \n",
       "2023-12-27  163.139999  163.639999  162.679993  163.460007  161.983231   \n",
       "2023-12-28  163.960007  163.960007  163.399994  163.750000  162.270599   \n",
       "2023-12-29  163.750000  164.179993  162.830002  163.550003  162.072403   \n",
       "\n",
       "             Volume  \n",
       "Date                 \n",
       "2019-01-02  4434935  \n",
       "2019-01-03  4546648  \n",
       "2019-01-04  4683779  \n",
       "2019-01-07  3923755  \n",
       "2019-01-08  4982726  \n",
       "...             ...  \n",
       "2023-12-22  2439800  \n",
       "2023-12-26  1772400  \n",
       "2023-12-27  3234600  \n",
       "2023-12-28  2071300  \n",
       "2023-12-29  2525600  \n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Sequences for RNN \n",
    "The way I think it makes sense right now is to have the features of the network be all available feature columns (Open, High, Low, Close, Adj Close, and Volume) and then as the label we are targeting be the next day's Abj Close. \n",
    "\n",
    "Once this is working the way that it needs to we can then set up a label for predicitng the previous day's. I think that it should be the same work flow. For a feature datapoint we are just trying to predict Adj Close -1 vs Adj Close +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(data)-seq_length-1):\n",
    "        x = data[i:(i+seq_length)]\n",
    "        y = data[i+seq_length][4] \n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "seq_length = 5\n",
    "X, y = create_sequences(scaled_features, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1252, 5, 6) (1252,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.8\n",
    "split = int(split_fraction * len(X))\n",
    "\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training set: (1001, 5, 6)\n",
      "y Training set: (1001,)\n"
     ]
    }
   ],
   "source": [
    "print(f'X Training set: {X_train.shape}\\ny Training set: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Torch Tensors and set up Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "batch_size = 64  \n",
    "\n",
    "train_data = TensorDataset(torch.Tensor(X_train), torch.Tensor(y_train))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(torch.Tensor(X_test), torch.Tensor(y_test))\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.9102, -2.0093, -1.9426, -2.0510, -2.0295, -0.1346],\n",
       "         [-1.8767, -1.7769, -1.7908, -1.6642, -1.8162, -0.0893],\n",
       "         [-1.6489, -1.6576, -1.5973, -1.5913, -1.7761, -0.3401],\n",
       "         [-1.4588, -1.5026, -1.3968, -1.4440, -1.6948,  0.0093],\n",
       "         [-1.3488, -1.4287, -1.3196, -1.3685, -1.6532, -0.3807]]),\n",
       " tensor(-1.6000))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the RNN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
    "        # Forward propagate the RNN\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # Pass the output of the last time step to the classifier\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define instance of model/training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleRNN(\n",
       "  (rnn): RNN(6, 20, batch_first=True)\n",
       "  (fc): Linear(in_features=20, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device == \"cuda\":\n",
    "    print(\"Running on GPU\")\n",
    "else: print(\"Running on CPU\")\n",
    "\n",
    "#Model Define\n",
    "model = SimpleRNN(input_size=6, hidden_size=20, output_size=1).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.5706\n",
      "Epoch [2/100], Loss: 0.2600\n",
      "Epoch [3/100], Loss: 0.1263\n",
      "Epoch [4/100], Loss: 0.0891\n",
      "Epoch [5/100], Loss: 0.0919\n",
      "Epoch [6/100], Loss: 0.0867\n",
      "Epoch [7/100], Loss: 0.0434\n",
      "Epoch [8/100], Loss: 0.0218\n",
      "Epoch [9/100], Loss: 0.0434\n",
      "Epoch [10/100], Loss: 0.0243\n",
      "Epoch [11/100], Loss: 0.0278\n",
      "Epoch [12/100], Loss: 0.0394\n",
      "Epoch [13/100], Loss: 0.0325\n",
      "Epoch [14/100], Loss: 0.0227\n",
      "Epoch [15/100], Loss: 0.0168\n",
      "Epoch [16/100], Loss: 0.0468\n",
      "Epoch [17/100], Loss: 0.0193\n",
      "Epoch [18/100], Loss: 0.0237\n",
      "Epoch [19/100], Loss: 0.0408\n",
      "Epoch [20/100], Loss: 0.0193\n",
      "Epoch [21/100], Loss: 0.0090\n",
      "Epoch [22/100], Loss: 0.0155\n",
      "Epoch [23/100], Loss: 0.0175\n",
      "Epoch [24/100], Loss: 0.0070\n",
      "Epoch [25/100], Loss: 0.0286\n",
      "Epoch [26/100], Loss: 0.0136\n",
      "Epoch [27/100], Loss: 0.0123\n",
      "Epoch [28/100], Loss: 0.0168\n",
      "Epoch [29/100], Loss: 0.0124\n",
      "Epoch [30/100], Loss: 0.0093\n",
      "Epoch [31/100], Loss: 0.0176\n",
      "Epoch [32/100], Loss: 0.0228\n",
      "Epoch [33/100], Loss: 0.0114\n",
      "Epoch [34/100], Loss: 0.0184\n",
      "Epoch [35/100], Loss: 0.0356\n",
      "Epoch [36/100], Loss: 0.0078\n",
      "Epoch [37/100], Loss: 0.0150\n",
      "Epoch [38/100], Loss: 0.0124\n",
      "Epoch [39/100], Loss: 0.0151\n",
      "Epoch [40/100], Loss: 0.0190\n",
      "Epoch [41/100], Loss: 0.0124\n",
      "Epoch [42/100], Loss: 0.0282\n",
      "Epoch [43/100], Loss: 0.0227\n",
      "Epoch [44/100], Loss: 0.0158\n",
      "Epoch [45/100], Loss: 0.0162\n",
      "Epoch [46/100], Loss: 0.0189\n",
      "Epoch [47/100], Loss: 0.0124\n",
      "Epoch [48/100], Loss: 0.0088\n",
      "Epoch [49/100], Loss: 0.0329\n",
      "Epoch [50/100], Loss: 0.0195\n",
      "Epoch [51/100], Loss: 0.0173\n",
      "Epoch [52/100], Loss: 0.0099\n",
      "Epoch [53/100], Loss: 0.0149\n",
      "Epoch [54/100], Loss: 0.0067\n",
      "Epoch [55/100], Loss: 0.0159\n",
      "Epoch [56/100], Loss: 0.0093\n",
      "Epoch [57/100], Loss: 0.0097\n",
      "Epoch [58/100], Loss: 0.0132\n",
      "Epoch [59/100], Loss: 0.0115\n",
      "Epoch [60/100], Loss: 0.0135\n",
      "Epoch [61/100], Loss: 0.0183\n",
      "Epoch [62/100], Loss: 0.0109\n",
      "Epoch [63/100], Loss: 0.0069\n",
      "Epoch [64/100], Loss: 0.0103\n",
      "Epoch [65/100], Loss: 0.0069\n",
      "Epoch [66/100], Loss: 0.0111\n",
      "Epoch [67/100], Loss: 0.0211\n",
      "Epoch [68/100], Loss: 0.0165\n",
      "Epoch [69/100], Loss: 0.0137\n",
      "Epoch [70/100], Loss: 0.0145\n",
      "Epoch [71/100], Loss: 0.0166\n",
      "Epoch [72/100], Loss: 0.0216\n",
      "Epoch [73/100], Loss: 0.0145\n",
      "Epoch [74/100], Loss: 0.0099\n",
      "Epoch [75/100], Loss: 0.0113\n",
      "Epoch [76/100], Loss: 0.0137\n",
      "Epoch [77/100], Loss: 0.0076\n",
      "Epoch [78/100], Loss: 0.0183\n",
      "Epoch [79/100], Loss: 0.0100\n",
      "Epoch [80/100], Loss: 0.0193\n",
      "Epoch [81/100], Loss: 0.0099\n",
      "Epoch [82/100], Loss: 0.0255\n",
      "Epoch [83/100], Loss: 0.0154\n",
      "Epoch [84/100], Loss: 0.0175\n",
      "Epoch [85/100], Loss: 0.0207\n",
      "Epoch [86/100], Loss: 0.0073\n",
      "Epoch [87/100], Loss: 0.0125\n",
      "Epoch [88/100], Loss: 0.0331\n",
      "Epoch [89/100], Loss: 0.0101\n",
      "Epoch [90/100], Loss: 0.0130\n",
      "Epoch [91/100], Loss: 0.0106\n",
      "Epoch [92/100], Loss: 0.0148\n",
      "Epoch [93/100], Loss: 0.0089\n",
      "Epoch [94/100], Loss: 0.0097\n",
      "Epoch [95/100], Loss: 0.0137\n",
      "Epoch [96/100], Loss: 0.0230\n",
      "Epoch [97/100], Loss: 0.0094\n",
      "Epoch [98/100], Loss: 0.0104\n",
      "Epoch [99/100], Loss: 0.0077\n",
      "Epoch [100/100], Loss: 0.0094\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, labels.unsqueeze(-1))\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.1522\n",
      "Mean Squared Error: 0.0532\n",
      "Root Mean Squared Error: 0.2307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "\n",
    "actuals = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        actuals.extend(labels.cpu().numpy())\n",
    "        predictions.extend(outputs.cpu().numpy())\n",
    "\n",
    "actuals = np.array(actuals)\n",
    "predictions = np.array(predictions)\n",
    "\n",
    "mae = mean_absolute_error(actuals, predictions)\n",
    "mse = mean_squared_error(actuals, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae:.4f}')\n",
    "print(f'Mean Squared Error: {mse:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics? \n",
    "Not sure what we want to use for evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1864595 , 1.1760012 , 1.2166018 , 1.2811924 , 1.1895367 ,\n",
       "       1.3488573 , 1.3396318 , 1.416524  , 1.4448221 , 1.4626597 ,\n",
       "       1.4835755 , 1.440514  , 1.1464753 , 1.1593944 , 1.1950716 ,\n",
       "       1.2356724 , 1.2129112 , 1.1680045 , 0.77984935, 0.77615774,\n",
       "       0.83213675, 0.797073  , 0.81921816, 0.8991866 , 0.93302053,\n",
       "       0.8862685 , 0.8653547 , 0.8739658 , 0.8378497 , 0.9530494 ,\n",
       "       1.0620219 , 0.97857946, 1.0028647 , 0.91568696, 0.91693276,\n",
       "       0.7108184 , 0.664739  , 0.65352947, 0.6398304 , 0.63484925,\n",
       "       0.56074727, 0.4916274 , 0.5377068 , 0.58191955, 0.61616707,\n",
       "       0.49536374, 0.48290992, 0.3652191 , 0.32100734, 0.32910293,\n",
       "       0.2711911 , 0.18588105, 0.27430454, 0.21141207, 0.35151997,\n",
       "       0.3907501 , 0.23382914, 0.19148557, 0.3110441 , 0.56136966,\n",
       "       0.5632383 , 0.5862793 , 0.55576617, 0.6722102 , 0.73261213,\n",
       "       0.70396817, 0.73759425, 0.63547164, 0.6684739 , 0.63048905,\n",
       "       0.5134216 , 0.4735686 , 0.48851395, 0.46858695, 0.46609688,\n",
       "       0.37518182, 0.3776729 , 0.33844328, 0.3178939 , 0.34840605,\n",
       "       0.34591547, 0.41565824, 0.38078687, 0.36086035, 0.302949  ,\n",
       "       0.1964667 , 0.14166881, 0.20892102, 0.19335325, 0.15737528,\n",
       "       0.21102631, 0.14033355, 0.26278383, 0.29560658, 0.301918  ,\n",
       "       0.44393632, 0.47170934, 0.54177123, 0.55691993, 0.5998407 ,\n",
       "       0.4420433 , 0.5102117 , 0.6446554 , 0.6818961 , 0.62571967,\n",
       "       0.7033567 , 0.8674665 , 0.881353  , 0.8845087 , 0.9911794 ,\n",
       "       0.9930744 , 1.0492508 , 1.1199441 , 1.1944244 , 1.1691767 ,\n",
       "       1.2449187 , 1.1868492 , 1.0909094 , 0.94762784, 0.7885673 ,\n",
       "       0.67873937, 0.7992966 , 0.86241674, 0.82580745, 0.9709825 ,\n",
       "       0.955202  , 0.9463662 , 0.9823436 , 0.8510566 , 0.8460064 ,\n",
       "       0.89776385, 0.99496746, 0.89397585, 0.9621457 , 0.92932296,\n",
       "       0.9823436 , 1.0530369 , 1.060611  , 1.243657  , 1.2790041 ,\n",
       "       1.3168746 , 1.3667396 , 1.4134479 , 1.5333748 , 1.5636712 ,\n",
       "       1.6097481 , 1.556097  , 1.6091177 , 1.6267903 , 1.6135361 ,\n",
       "       1.7359865 , 1.7189451 , 1.6065775 , 1.6550993 , 1.6467993 ,\n",
       "       1.5695461 , 1.5669919 , 1.4884621 , 1.4897387 , 1.5376232 ,\n",
       "       1.5931692 , 1.5427315 , 1.665315  , 1.6742532 , 1.7891752 ,\n",
       "       1.8319522 , 1.859405  , 1.8855814 , 1.8836664 , 1.9545346 ,\n",
       "       1.9666654 , 1.9621962 , 1.9277191 , 1.9379349 , 1.9826268 ,\n",
       "       1.8498286 , 1.86579   , 1.9168661 , 1.8300363 , 1.7725743 ,\n",
       "       1.8638752 , 2.0752032 , 1.9187809 , 1.8887749 , 1.86132   ,\n",
       "       1.6544609 , 1.6499909 , 1.5484762 , 1.4667549 , 1.4986769 ,\n",
       "       1.4725006 , 1.515916  , 1.5446465 , 1.5772077 , 1.5880609 ,\n",
       "       1.5823151 , 1.6538227 , 1.5267701 , 1.3492799 , 1.3971633 ,\n",
       "       1.4680315 , 1.445686  , 1.3205473 , 1.2662789 , 1.2164795 ,\n",
       "       1.3065017 , 1.2611725 , 1.6876606 , 1.6084924 , 1.6155157 ,\n",
       "       1.7438449 , 1.7923666 , 1.8951578 , 1.9519805 , 2.0202956 ,\n",
       "       2.0113573 , 1.9602814 , 1.9764228 , 2.1313906 , 2.071987  ,\n",
       "       2.2211423 , 2.3612592 , 2.3922515 , 2.3812752 , 2.4755478 ,\n",
       "       2.4471364 , 2.5259118 , 2.5291393 , 2.5543222 , 2.5594873 ,\n",
       "       2.6085608 , 2.7473853 , 2.8758795 , 2.9113927 , 2.9301171 ,\n",
       "       2.8584445 , 2.8545709 , 2.9669228 , 3.0670047 , 3.1444895 ,\n",
       "       3.0741074 , 3.028263  , 2.9843557 , 3.0172868 , 2.9410944 ,\n",
       "       2.8435946 , 2.8907294 , 2.9785445 , 3.0476348 , 3.0637774 ,\n",
       "       3.082502  ], dtype=float32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1240916 ],\n",
       "       [ 1.1663694 ],\n",
       "       [ 1.129085  ],\n",
       "       [ 1.1938637 ],\n",
       "       [ 1.2558901 ],\n",
       "       [ 1.1665697 ],\n",
       "       [ 1.307838  ],\n",
       "       [ 1.323579  ],\n",
       "       [ 1.3587172 ],\n",
       "       [ 1.3849747 ],\n",
       "       [ 1.4227203 ],\n",
       "       [ 1.4146271 ],\n",
       "       [ 1.4175854 ],\n",
       "       [ 1.165248  ],\n",
       "       [ 1.1585833 ],\n",
       "       [ 1.1709232 ],\n",
       "       [ 1.2343991 ],\n",
       "       [ 1.1923956 ],\n",
       "       [ 1.1511716 ],\n",
       "       [ 0.7219816 ],\n",
       "       [ 0.78020006],\n",
       "       [ 0.8470139 ],\n",
       "       [ 0.8804187 ],\n",
       "       [ 0.8327758 ],\n",
       "       [ 0.9089691 ],\n",
       "       [ 0.9451005 ],\n",
       "       [ 0.87511075],\n",
       "       [ 0.8691942 ],\n",
       "       [ 0.87501425],\n",
       "       [ 0.7949423 ],\n",
       "       [ 0.90815806],\n",
       "       [ 1.0413985 ],\n",
       "       [ 0.9622765 ],\n",
       "       [ 0.9707163 ],\n",
       "       [ 0.8961257 ],\n",
       "       [ 0.89811033],\n",
       "       [ 0.7156938 ],\n",
       "       [ 0.66235584],\n",
       "       [ 0.62851906],\n",
       "       [ 0.6173052 ],\n",
       "       [ 0.63694733],\n",
       "       [ 0.5706381 ],\n",
       "       [ 0.5025795 ],\n",
       "       [ 0.5190586 ],\n",
       "       [ 0.5842765 ],\n",
       "       [ 0.61342955],\n",
       "       [ 0.50548434],\n",
       "       [ 0.47595578],\n",
       "       [ 0.3807106 ],\n",
       "       [ 0.3546798 ],\n",
       "       [ 0.3651426 ],\n",
       "       [ 0.3293766 ],\n",
       "       [ 0.22212568],\n",
       "       [ 0.29708585],\n",
       "       [-0.00821785],\n",
       "       [ 0.17324251],\n",
       "       [ 0.41617522],\n",
       "       [ 0.36949492],\n",
       "       [ 0.13304555],\n",
       "       [ 0.28245103],\n",
       "       [ 0.5588821 ],\n",
       "       [ 0.590237  ],\n",
       "       [ 0.5811256 ],\n",
       "       [ 0.56911635],\n",
       "       [ 0.6540815 ],\n",
       "       [ 0.72795236],\n",
       "       [ 0.6971817 ],\n",
       "       [ 0.7313324 ],\n",
       "       [ 0.6339823 ],\n",
       "       [ 0.63422   ],\n",
       "       [ 0.6154164 ],\n",
       "       [ 0.523617  ],\n",
       "       [ 0.45811516],\n",
       "       [ 0.51006454],\n",
       "       [ 0.46796888],\n",
       "       [ 0.4826451 ],\n",
       "       [ 0.37437168],\n",
       "       [ 0.46932042],\n",
       "       [ 0.38502023],\n",
       "       [ 0.35581565],\n",
       "       [ 0.3796712 ],\n",
       "       [ 0.3784462 ],\n",
       "       [ 0.41171005],\n",
       "       [ 0.38918003],\n",
       "       [ 0.37727776],\n",
       "       [ 0.2889362 ],\n",
       "       [ 0.22941697],\n",
       "       [ 0.14503053],\n",
       "       [ 0.2207933 ],\n",
       "       [ 0.21360803],\n",
       "       [ 0.12274129],\n",
       "       [ 0.20023882],\n",
       "       [ 0.14765704],\n",
       "       [ 0.25685203],\n",
       "       [ 0.28832698],\n",
       "       [ 0.28593647],\n",
       "       [ 0.41616976],\n",
       "       [ 0.45461726],\n",
       "       [ 0.5437351 ],\n",
       "       [ 0.5530936 ],\n",
       "       [ 0.5856493 ],\n",
       "       [ 0.45325416],\n",
       "       [ 0.4885913 ],\n",
       "       [ 0.63185835],\n",
       "       [ 0.68593407],\n",
       "       [ 0.6246393 ],\n",
       "       [ 0.72652584],\n",
       "       [ 0.8344063 ],\n",
       "       [ 0.9220424 ],\n",
       "       [ 0.85114837],\n",
       "       [ 0.95534617],\n",
       "       [ 1.0017319 ],\n",
       "       [ 1.0211265 ],\n",
       "       [ 1.103095  ],\n",
       "       [ 1.156169  ],\n",
       "       [ 1.1528994 ],\n",
       "       [ 1.2075034 ],\n",
       "       [ 1.1767274 ],\n",
       "       [ 1.0873997 ],\n",
       "       [ 0.9306517 ],\n",
       "       [ 0.804698  ],\n",
       "       [ 0.6972413 ],\n",
       "       [ 0.82925636],\n",
       "       [ 0.8453894 ],\n",
       "       [ 0.8336135 ],\n",
       "       [ 0.92041045],\n",
       "       [ 0.9417514 ],\n",
       "       [ 0.9106341 ],\n",
       "       [ 0.9172341 ],\n",
       "       [ 0.83546823],\n",
       "       [ 0.8309923 ],\n",
       "       [ 0.84642565],\n",
       "       [ 0.9552068 ],\n",
       "       [ 0.88813883],\n",
       "       [ 0.9474705 ],\n",
       "       [ 0.89307284],\n",
       "       [ 0.94895023],\n",
       "       [ 1.0251147 ],\n",
       "       [ 1.0539637 ],\n",
       "       [ 1.20635   ],\n",
       "       [ 1.243128  ],\n",
       "       [ 1.2711891 ],\n",
       "       [ 1.3137296 ],\n",
       "       [ 1.3557118 ],\n",
       "       [ 1.468374  ],\n",
       "       [ 1.4949278 ],\n",
       "       [ 1.5312483 ],\n",
       "       [ 1.4842201 ],\n",
       "       [ 1.5165964 ],\n",
       "       [ 1.5396147 ],\n",
       "       [ 1.5458109 ],\n",
       "       [ 1.6245456 ],\n",
       "       [ 1.5997523 ],\n",
       "       [ 1.495554  ],\n",
       "       [ 1.5468966 ],\n",
       "       [ 1.524371  ],\n",
       "       [ 1.485366  ],\n",
       "       [ 1.4812275 ],\n",
       "       [ 1.4168668 ],\n",
       "       [ 1.42826   ],\n",
       "       [ 1.4345884 ],\n",
       "       [ 1.4872763 ],\n",
       "       [ 1.4646356 ],\n",
       "       [ 1.5397383 ],\n",
       "       [ 1.5619239 ],\n",
       "       [ 1.6440536 ],\n",
       "       [ 1.6882135 ],\n",
       "       [ 1.7065148 ],\n",
       "       [ 1.720513  ],\n",
       "       [ 1.7379341 ],\n",
       "       [ 1.7798733 ],\n",
       "       [ 1.7941726 ],\n",
       "       [ 1.7850862 ],\n",
       "       [ 1.7753514 ],\n",
       "       [ 1.7776141 ],\n",
       "       [ 1.8054721 ],\n",
       "       [ 1.7153568 ],\n",
       "       [ 1.7193862 ],\n",
       "       [ 1.7509999 ],\n",
       "       [ 1.6992794 ],\n",
       "       [ 1.655556  ],\n",
       "       [ 1.6926072 ],\n",
       "       [ 1.836181  ],\n",
       "       [ 1.7520018 ],\n",
       "       [ 1.7384015 ],\n",
       "       [ 1.7142993 ],\n",
       "       [ 1.568068  ],\n",
       "       [ 1.5488567 ],\n",
       "       [ 1.4504534 ],\n",
       "       [ 1.4119084 ],\n",
       "       [ 1.4146913 ],\n",
       "       [ 1.395665  ],\n",
       "       [ 1.4239743 ],\n",
       "       [ 1.4417984 ],\n",
       "       [ 1.4714003 ],\n",
       "       [ 1.4741842 ],\n",
       "       [ 1.4913083 ],\n",
       "       [ 1.5363073 ],\n",
       "       [ 1.4450978 ],\n",
       "       [ 1.299543  ],\n",
       "       [ 1.3362919 ],\n",
       "       [ 1.3308171 ],\n",
       "       [ 1.377106  ],\n",
       "       [ 1.2704382 ],\n",
       "       [ 1.2406366 ],\n",
       "       [ 1.1646808 ],\n",
       "       [ 1.2370089 ],\n",
       "       [ 1.2216656 ],\n",
       "       [ 1.56503   ],\n",
       "       [ 1.5091498 ],\n",
       "       [ 1.5322322 ],\n",
       "       [ 1.6047482 ],\n",
       "       [ 1.6597415 ],\n",
       "       [ 1.7178867 ],\n",
       "       [ 1.7818689 ],\n",
       "       [ 1.8239719 ],\n",
       "       [ 1.8240172 ],\n",
       "       [ 1.799711  ],\n",
       "       [ 1.7456682 ],\n",
       "       [ 1.8677162 ],\n",
       "       [ 1.8393525 ],\n",
       "       [ 1.9480559 ],\n",
       "       [ 2.0171847 ],\n",
       "       [ 2.0524216 ],\n",
       "       [ 2.0573745 ],\n",
       "       [ 2.092128  ],\n",
       "       [ 2.0953457 ],\n",
       "       [ 2.138295  ],\n",
       "       [ 2.1338487 ],\n",
       "       [ 2.1537294 ],\n",
       "       [ 2.1547308 ],\n",
       "       [ 2.1889882 ],\n",
       "       [ 2.2234032 ],\n",
       "       [ 2.2770917 ],\n",
       "       [ 2.3092542 ],\n",
       "       [ 2.3221695 ],\n",
       "       [ 2.3104873 ],\n",
       "       [ 2.3102596 ],\n",
       "       [ 2.3349302 ],\n",
       "       [ 2.3643184 ],\n",
       "       [ 2.3954935 ],\n",
       "       [ 2.3837984 ],\n",
       "       [ 2.3527129 ],\n",
       "       [ 2.288045  ],\n",
       "       [ 2.3401086 ],\n",
       "       [ 2.3479047 ],\n",
       "       [ 2.303058  ],\n",
       "       [ 2.3154464 ],\n",
       "       [ 2.3569772 ],\n",
       "       [ 2.38429   ],\n",
       "       [ 2.3928306 ]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predictpast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
